{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVEwjMHq8bli",
        "outputId": "083c3957-de5c-4657-ca89-1086b2091241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#Conexão com o Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "W5XK4Eg88Nnr",
        "outputId": "2c4c07d8-6728-4ce4-a30b-e293ea0d01ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotADirectoryError",
          "evalue": "[Errno 20] Not a directory: '/content/CNN_Links.txt/CNN_Links.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4234733375.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# Chamada da função para extrair os conteúdos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mget_news_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/CNN_Links.txt/CNN_Links.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-4234733375.py\u001b[0m in \u001b[0;36mget_news_content\u001b[0;34m(links_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_news_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Carrega o arquivo com os links e retorna uma lista\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinks_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mlinks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnews_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: '/content/CNN_Links.txt/CNN_Links.txt'"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "\n",
        "def get_news_content(links_file):\n",
        "    # Carrega o arquivo com os links e retorna uma lista\n",
        "    with open(links_file, 'r') as file:\n",
        "        links = set(file.readlines())\n",
        "    news_contents = []\n",
        "\n",
        "    for link in links:\n",
        "        link = link.strip()  # Limpa a URL, se necessário\n",
        "        print(link)\n",
        "        response = requests.get(link)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Localiza o elemento que contém o conteúdo do artigo\n",
        "            article_content = soup.find('div', class_='article__content')\n",
        "            if article_content:\n",
        "                paragraphs = article_content.find_all('p', class_='paragraph inline-placeholder')\n",
        "                # Concatena o texto de todos os parágrafos encontrados\n",
        "                content = ' '.join(p.get_text(strip=True) for p in paragraphs)\n",
        "                news_contents.append(content)\n",
        "                print(content)\n",
        "            else:\n",
        "                news_contents.append(\"Conteúdo não encontrado.\")\n",
        "        else:\n",
        "            news_contents.append(f\"Falha ao extrair o conteúdo da notícia: {response.status_code}\")\n",
        "\n",
        "    # Salva o conteúdo em um arquivo JSON\n",
        "    with open('news_contents.json', 'w') as json_file:\n",
        "        json.dump({\"news_content\": news_contents}, json_file)\n",
        "\n",
        "# Chamada da função para extrair os conteúdos\n",
        "get_news_content('/content/CNN_Links.txt')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}